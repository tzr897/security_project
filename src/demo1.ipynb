{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_1.py is OK for milestone1\n",
    "\n",
    "This file is still being improved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bdth333/.local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow\n",
    "from tflearn.data_utils import VocabularyProcessor\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "enron_data_path = \"../dataset/enron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_set(data_path):\n",
    "    res = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file_name in files:\n",
    "            cur_file_path = os.path.join(root, file_name)\n",
    "            cur_txt = \"\"\n",
    "            with open(cur_file_path) as cur_f:\n",
    "                try:\n",
    "                    cur_lines = cur_f.readlines()\n",
    "                    for one_line in cur_lines:\n",
    "                        one_line = one_line.strip('\\n')\n",
    "                        one_line = one_line.strip('\\r')\n",
    "                        cur_txt += one_line\n",
    "                    res.append(cur_txt)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "    return res\n",
    "\n",
    "def process_all_data():\n",
    "    spam_list = []\n",
    "    ham_list = []\n",
    "    NUM_SET = 1\n",
    "    for i in range(1, NUM_SET + 1):\n",
    "        spam_path = enron_data_path + str(i) +  \"/spam/\"\n",
    "        ham_path = enron_data_path + str(i) + \"/ham/\"\n",
    "        cur_spam_list = preprocess_one_set(spam_path)\n",
    "        cur_ham_list = preprocess_one_set(ham_path)\n",
    "        spam_list += cur_spam_list\n",
    "        ham_list += cur_ham_list\n",
    "    return spam_list, ham_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of spam_list:  1487\n",
      "len of ham_list:  3672\n"
     ]
    }
   ],
   "source": [
    "spam_list, ham_list = process_all_data()\n",
    "emails_list = spam_list + ham_list\n",
    "len_spam = len(spam_list)\n",
    "len_ham = len(ham_list)\n",
    "print(\"len of spam_list: \", len_spam)\n",
    "print(\"len of ham_list: \", len_ham)\n",
    "y = [1] * len_spam + [0] * len_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_bagofwords(emails_list, num_features):\n",
    "    # currently do not consider max_features\n",
    "    tfidfv = TfidfVectorizer(\n",
    "        decode_error = \"ignore\",\n",
    "        analyzer = \"word\",\n",
    "        stop_words = \"english\",\n",
    "        smooth_idf = False,\n",
    "        max_features = num_features,# at most 2010 features for two datasets\n",
    "    )\n",
    "    x = np.array(emails_list)\n",
    "    x = tfidfv.fit_transform(x)\n",
    "    x = x.toarray()\n",
    "    # vocab_path = \"../output/vocabulary_bagofwords.txt\"\n",
    "    # print(\"output the vocabulary to \" + vocab_path + \" ......\\n\")\n",
    "    # with open(vocab_path, 'w') as f:\n",
    "    #     f.write(json.dumps(tfidfv.vocabulary_))\n",
    "    print(\"len of x: \", len(x))\n",
    "    print(\"#features: \",len(x[0]))\n",
    "    return x\n",
    "\n",
    "\n",
    "def feature_extraction_vo(emails_list):\n",
    "    vp = VocabularyProcessor(\n",
    "        max_document_length = 1000,\n",
    "        min_frequency = 1,\n",
    "        vocabulary = None,\n",
    "        tokenizer_fn = None\n",
    "    )\n",
    "    x = vp.fit_transform(emails_list)\n",
    "    x = np.array(list(x))\n",
    "    vocab_path = \"../output/vocabulary_tf.txt\"\n",
    "    # with open(vocab_path, 'w') as f:\n",
    "    #     f.write(json.dumps(vp.vocabulary_._mapping))\n",
    "    print(\"len of x: \", len(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def detection_svm(X_train, y_train, X_test, y_test):\n",
    "    print(\"---Support Vector Machines Algorithm---\")\n",
    "    start_time = time.time()\n",
    "    svm_c = svm.SVC()\n",
    "    svm_c.fit(X_train, y_train)\n",
    "    y_pred = svm_c.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    t = end_time - start_time\n",
    "    res = [t, precision, recall, f1, accuracy]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest Neighbor Algorithm (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def detection_knn(X_train, y_train, X_test, y_test):\n",
    "    print(\"---K-nearest Neighbor Algorithm (KNN)---\")\n",
    "    start_time = time.time()\n",
    "    knnc = KNeighborsClassifier()\n",
    "    knnc.fit(X_train, y_train)\n",
    "    y_pred = knnc.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    t = end_time - start_time\n",
    "    res = [t, precision, recall, f1, accuracy]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def detection_gnb(X_train, y_train, X_test, y_test):\n",
    "    print(\"---Gaussian Naive Bayes---\")\n",
    "    start_time = time.time()\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    t = end_time - start_time\n",
    "    res = [t, precision, recall, f1, accuracy]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def detection_kmeans(X_train, y_train, X_test, y_test, num_clusters):\n",
    "    print(\"---K-means Clustering---\")\n",
    "    start_time = time.time()\n",
    "    kmc = KMeans(n_clusters = num_clusters)\n",
    "    kmc.fit(X_train)\n",
    "    y_pred = kmc.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(y_pred)\n",
    "    y_processed = []\n",
    "    for yi in y_pred:\n",
    "        if yi == 0:\n",
    "            y_processed.append(0)\n",
    "        else:\n",
    "            y_processed.append(1)\n",
    "    precision = metrics.precision_score(y_test, y_processed)\n",
    "    recall = metrics.recall_score(y_test, y_processed)\n",
    "    f1 = metrics.f1_score(y_test, y_processed)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_processed)\n",
    "    t = end_time - start_time\n",
    "    res = [t, precision, recall, f1, accuracy]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def detection_rf(X_train, y_train, X_test, y_test):\n",
    "    print(\"---Random Forest---\")\n",
    "    start_time = time.time()\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "    end_time = time.time()\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    t = end_time - start_time\n",
    "    res = [t, precision, recall, f1, accuracy]\n",
    "    return res\n",
    "    # print(\"precision: \", precision)\n",
    "    # print(\"recall: \", recall)\n",
    "    # print(\"f1: \", f1)\n",
    "    # print(\"accuracy: \", accuracy)\n",
    "    # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x:  5159\n",
      "#features:  1000\n",
      "len of x:  5159\n",
      "#features:  2000\n",
      "len of x:  5159\n",
      "#features:  3000\n",
      "len of x:  5159\n",
      "#features:  4000\n",
      "len of x:  5159\n",
      "#features:  5000\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "---Gaussian Naive Bayes---\n",
      "---K-means Clustering---\n",
      "---Random Forest---\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "---Gaussian Naive Bayes---\n",
      "---K-means Clustering---\n",
      "---Random Forest---\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "---Gaussian Naive Bayes---\n",
      "---K-means Clustering---\n",
      "---Random Forest---\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "---Gaussian Naive Bayes---\n",
      "---K-means Clustering---\n",
      "---Random Forest---\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "---Gaussian Naive Bayes---\n",
      "---K-means Clustering---\n",
      "---Random Forest---\n",
      "{1000: [[0.867828369140625, 0.9243243243243243, 0.9144385026737968, 0.9193548387096774, 0.9534883720930233], [0.07200360298156738, 0.8893129770992366, 0.9344919786096256, 0.9113428943937418, 0.9472868217054263], [0.803351879119873, 0.0, 0.0, 0.0, 0.5891472868217055], [0.8132867813110352, 0.9266409266409267, 0.9625668449197861, 0.9442622950819672, 0.9670542635658915]], 2000: [[1.4417965412139893, 0.9393939393939394, 0.9117647058823529, 0.9253731343283583, 0.9573643410852714], [0.15820765495300293, 0.9316939890710383, 0.9117647058823529, 0.9216216216216216, 0.9550387596899225], [1.6624128818511963, 0.32778264680105174, 1.0, 0.49372937293729374, 0.4054263565891473], [1.3528552055358887, 0.9371794871794872, 0.9772727272727273, 0.9568062827225131, 0.9744186046511628]], 3000: [[1.9524970054626465, 0.9644381223328592, 0.9064171122994652, 0.9345279117849759, 0.9631782945736435], [0.2423560619354248, 0.943502824858757, 0.893048128342246, 0.9175824175824177, 0.9534883720930233], [2.3341124057769775, 0.0, 0.0, 0.0, 0.5961240310077519], [2.157081365585327, 0.9369369369369369, 0.9732620320855615, 0.9547540983606557, 0.9732558139534884]], 4000: [[3.1860861778259277, 0.9679218967921897, 0.9278074866310161, 0.9474402730375426, 0.9701550387596899], [0.3355867862701416, 0.9411764705882353, 0.8983957219251337, 0.9192886456908345, 0.9542635658914729], [2.3781142234802246, 0.0, 0.0, 0.0, 0.5976744186046512], [2.650974988937378, 0.9406451612903226, 0.9745989304812834, 0.9573210768220617, 0.9748062015503876]], 5000: [[3.3294918537139893, 0.9720670391061452, 0.93048128342246, 0.9508196721311476, 0.9720930232558139], [0.40402960777282715, 0.9343575418994413, 0.8943850267379679, 0.9139344262295082, 0.9511627906976744], [3.0318846702575684, 0.3260680034873583, 1.0, 0.49178172255095337, 0.4007751937984496], [3.220362424850464, 0.9335038363171355, 0.9759358288770054, 0.9542483660130718, 0.9728682170542635]]}\n"
     ]
    }
   ],
   "source": [
    "NUM_FEATURE_ARR = [1000, 2000, 3000, 4000, 5000]\n",
    "TEST_SIZE = 0.5\n",
    "\n",
    "def bagofwords_split():\n",
    "    X_train_bagofwords_arr = []\n",
    "    X_test_bagofwords_arr = []\n",
    "    y_train_bagofwords_arr = []\n",
    "    y_test_bagofwords_arr = []\n",
    "\n",
    "    for num_feature in NUM_FEATURE_ARR:\n",
    "        x = feature_extraction_bagofwords(emails_list, num_feature)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=0)\n",
    "        X_train_bagofwords_arr.append(X_train)\n",
    "        X_test_bagofwords_arr.append(X_test)\n",
    "        y_train_bagofwords_arr.append(y_train)\n",
    "        y_test_bagofwords_arr.append(y_test)\n",
    "    return X_train_bagofwords_arr, X_test_bagofwords_arr, y_train_bagofwords_arr, y_test_bagofwords_arr\n",
    "\n",
    "X_train_bagofwords_arr, X_test_bagofwords_arr, y_train_bagofwords_arr, y_test_bagofwords_arr = bagofwords_split()\n",
    "\n",
    "def onetest():\n",
    "    res = dict()\n",
    "    for i in range(0, len(NUM_FEATURE_ARR)):\n",
    "        #res_svm = detection_svm(X_train_bagofwords_arr[i], y_train_bagofwords_arr[i], X_test_bagofwords_arr[i], y_test_bagofwords_arr[i])\n",
    "        res_knn = detection_knn(X_train_bagofwords_arr[i], y_train_bagofwords_arr[i], X_test_bagofwords_arr[i], y_test_bagofwords_arr[i])\n",
    "        res_gnb = detection_gnb(X_train_bagofwords_arr[i], y_train_bagofwords_arr[i], X_test_bagofwords_arr[i], y_test_bagofwords_arr[i])\n",
    "        res_kmeans = detection_kmeans(X_train_bagofwords_arr[i], y_train_bagofwords_arr[i], X_test_bagofwords_arr[i], y_test_bagofwords_arr[i], 2)\n",
    "        res_rf = detection_rf(X_train_bagofwords_arr[i], y_train_bagofwords_arr[i], X_test_bagofwords_arr[i], y_test_bagofwords_arr[i])\n",
    "        temp = [res_knn, res_gnb, res_kmeans, res_rf]\n",
    "        res[NUM_FEATURE_ARR[i]] = temp\n",
    "    return res\n",
    "\n",
    "res = onetest()\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-77db74ef256e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mres_gnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_gnb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres_kmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_kmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mres_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "res_svm = detection_svm(X_train, y_train, X_test, y_test)\n",
    "res_knn = detection_knn(X_train, y_train, X_test, y_test)\n",
    "res_gnb = detection_gnb(X_train, y_train, X_test, y_test)\n",
    "res_kmeans = detection_kmeans(X_train, y_train, X_test, y_test, 2)\n",
    "res_rf = detection_rf(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x:  5159\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.4\n",
    "\n",
    "x = feature_extraction_vo(emails_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Support Vector Machines Algorithm---\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "---Gaussian Naive Bayes---\n",
      "---K-means Clustering---\n",
      "ss:  {0, 1}\n",
      "---Random Forest---\n"
     ]
    }
   ],
   "source": [
    "res_svm = detection_svm(X_train, y_train, X_test, y_test)\n",
    "res_knn = detection_knn(X_train, y_train, X_test, y_test)\n",
    "res_gnb = detection_gnb(X_train, y_train, X_test, y_test)\n",
    "res_kmeans = detection_kmeans(X_train, y_train, X_test, y_test, 2)\n",
    "res_rf = detection_rf(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.802137851715088, 0.8040201005025126, 0.27586206896551724, 0.41078305519897307, 0.7776162790697675]\n",
      "[0.7527022361755371, 0.5408560311284046, 0.2396551724137931, 0.3321385902031064, 0.7291666666666666]\n",
      "[0.06689810752868652, 0.4439252336448598, 0.16379310344827586, 0.23929471032745595, 0.7073643410852714]\n",
      "[1.0448384284973145, 1.0, 0.020689655172413793, 0.040540540540540536, 0.7248062015503876]\n",
      "[1.4990005493164062, 0.8758389261744967, 0.45, 0.5945330296127562, 0.8275193798449613]\n"
     ]
    }
   ],
   "source": [
    "print(res_svm)\n",
    "print(res_knn)\n",
    "print(res_gnb)\n",
    "print(res_kmeans)\n",
    "print(res_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv3",
   "language": "python",
   "name": "senv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
