{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_1.py is OK for milestone1\n",
    "\n",
    "This file is still being improved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/bdth333/.local/lib/python3.6/site-packages\")\n",
    "#sys.path.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from tflearn.data_utils import VocabularyProcessor\n",
    "\n",
    "#import tensorflow\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "enron_data_path = \"../dataset/enron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('len of spam_list: ', 1500)\n",
      "('len of ham_list: ', 3672)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_one_set(data_path):\n",
    "    res = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file_name in files:\n",
    "            cur_file_path = os.path.join(root, file_name)\n",
    "            cur_txt = \"\"\n",
    "            with open(cur_file_path) as cur_f:\n",
    "                try:\n",
    "                    cur_lines = cur_f.readlines()\n",
    "                    for one_line in cur_lines:\n",
    "                        one_line = one_line.strip('\\n')\n",
    "                        one_line = one_line.strip('\\r')\n",
    "                        cur_txt += one_line\n",
    "                    res.append(cur_txt)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "    return res\n",
    "\n",
    "def process_all_data():\n",
    "    spam_list = []\n",
    "    ham_list = []\n",
    "    NUM_SET = 1\n",
    "    for i in range(1, NUM_SET + 1):\n",
    "        spam_path = enron_data_path + str(i) +  \"/spam/\"\n",
    "        ham_path = enron_data_path + str(i) + \"/ham/\"\n",
    "        cur_spam_list = preprocess_one_set(spam_path)\n",
    "        cur_ham_list = preprocess_one_set(ham_path)\n",
    "        spam_list += cur_spam_list\n",
    "        ham_list += cur_ham_list\n",
    "    print(\"len of spam_list: \", len(spam_list))\n",
    "    print(\"len of ham_list: \", len(ham_list))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    return spam_list, ham_list\n",
    "\n",
    "spam_list, ham_list = process_all_data()\n",
    "emails_list = spam_list + ham_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_wordbags():\n",
    "    # currently do not consider max_features\n",
    "    tfidfv = TfidfVectorizer(\n",
    "        decode_error = \"ignore\",\n",
    "        analyzer = \"word\",\n",
    "        stop_words = \"english\",\n",
    "        smooth_idf = False,\n",
    "        #max_features = 2010,# at most 2010 features for two datasets\n",
    "    )\n",
    "    x = np.array(emails_list)\n",
    "    cnt = tfidfv.fit_transform(x)\n",
    "    cnt = cnt.toarray()\n",
    "    vocab_path = \"../output/vocabulary_wordbags.txt\"\n",
    "    print(\"output the vocabulary to \" + vocab_path + \" ......\\n\")\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        f.write(json.dumps(tfidfv.vocabulary_))\n",
    "    print(\"len of x: \", len(cnt))\n",
    "    print(\"#features: \",len(cnt[0]))\n",
    "\n",
    "\n",
    "def feature_extraction_vo():\n",
    "    vp = VocabularyProcessor(\n",
    "        max_document_length = 1000,\n",
    "        min_frequency = 1,\n",
    "        vocabulary = None,\n",
    "        tokenizer_fn = None\n",
    "    )\n",
    "    x = vp.fit_transform(emails_list)\n",
    "    x = np.array(list(x))\n",
    "    print(x)\n",
    "    vocab_path = \"../output/vocabulary_tf.txt\"\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        f.write(json.dumps(vp.vocabulary_._mapping))\n",
    "\n",
    "    print(\"len of x: \", len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output the vocabulary to ../output/vocabulary_wordbags.txt ......\n",
      "\n",
      "('len of x: ', 5172)\n",
      "('#features: ', 73321)\n"
     ]
    }
   ],
   "source": [
    "feature_extraction_wordbags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'VocabularyProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3e630165de2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_extraction_vo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f5784a0139cc>\u001b[0m in \u001b[0;36mfeature_extraction_vo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeature_extraction_vo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     vp = VocabularyProcessor(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmax_document_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmin_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'VocabularyProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "feature_extraction_vo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
