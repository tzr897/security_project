{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_1.py is OK for milestone1\n",
    "\n",
    "This file is still being improved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow\n",
    "from tflearn.data_utils import VocabularyProcessor\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "enron_data_path = \"../dataset/enron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_set(data_path):\n",
    "    res = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file_name in files:\n",
    "            cur_file_path = os.path.join(root, file_name)\n",
    "            cur_txt = \"\"\n",
    "            with open(cur_file_path) as cur_f:\n",
    "                try:\n",
    "                    cur_lines = cur_f.readlines()\n",
    "                    for one_line in cur_lines:\n",
    "                        one_line = one_line.strip('\\n')\n",
    "                        one_line = one_line.strip('\\r')\n",
    "                        cur_txt += one_line\n",
    "                    res.append(cur_txt)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "    return res\n",
    "\n",
    "def process_all_data():\n",
    "    spam_list = []\n",
    "    ham_list = []\n",
    "    NUM_SET = 1\n",
    "    for i in range(1, NUM_SET + 1):\n",
    "        spam_path = enron_data_path + str(i) +  \"/spam/\"\n",
    "        ham_path = enron_data_path + str(i) + \"/ham/\"\n",
    "        cur_spam_list = preprocess_one_set(spam_path)\n",
    "        cur_ham_list = preprocess_one_set(ham_path)\n",
    "        spam_list += cur_spam_list\n",
    "        ham_list += cur_ham_list\n",
    "    return spam_list, ham_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of spam_list:  1487\n",
      "len of ham_list:  3672\n"
     ]
    }
   ],
   "source": [
    "spam_list, ham_list = process_all_data()\n",
    "emails_list = spam_list + ham_list\n",
    "len_spam = len(spam_list)\n",
    "len_ham = len(ham_list)\n",
    "print(\"len of spam_list: \", len_spam)\n",
    "print(\"len of ham_list: \", len_ham)\n",
    "y = [1] * len_spam + [0] * len_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_bagofwords(emails_list, num_features):\n",
    "    # currently do not consider max_features\n",
    "    tfidfv = TfidfVectorizer(\n",
    "        decode_error = \"ignore\",\n",
    "        analyzer = \"word\",\n",
    "        stop_words = \"english\",\n",
    "        smooth_idf = False,\n",
    "        max_features = num_features,# at most 2010 features for two datasets\n",
    "    )\n",
    "    x = np.array(emails_list)\n",
    "    x = tfidfv.fit_transform(x)\n",
    "    x = x.toarray()\n",
    "    # vocab_path = \"../output/vocabulary_bagofwords.txt\"\n",
    "    # print(\"output the vocabulary to \" + vocab_path + \" ......\\n\")\n",
    "    # with open(vocab_path, 'w') as f:\n",
    "    #     f.write(json.dumps(tfidfv.vocabulary_))\n",
    "    print(\"len of x: \", len(x))\n",
    "    print(\"#features: \",len(x[0]))\n",
    "    return x\n",
    "\n",
    "\n",
    "def feature_extraction_vo(emails_list):\n",
    "    vp = VocabularyProcessor(\n",
    "        max_document_length = 1000,\n",
    "        min_frequency = 1,\n",
    "        vocabulary = None,\n",
    "        tokenizer_fn = None\n",
    "    )\n",
    "    x = vp.fit_transform(emails_list)\n",
    "    x = np.array(list(x))\n",
    "    vocab_path = \"../output/vocabulary_tf.txt\"\n",
    "    # with open(vocab_path, 'w') as f:\n",
    "    #     f.write(json.dumps(vp.vocabulary_._mapping))\n",
    "    print(\"len of x: \", len(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def detection_svm(X_train, y_train, X_test, y_test):\n",
    "    print(\"---Support Vector Machines Algorithm---\")\n",
    "    svm_c = svm.SVC()\n",
    "    svm_c.fit(X_train, y_train)\n",
    "    y_pred = svm_c.predict(X_test)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest Neighbor Algorithm (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def detection_knn(X_train, y_train, X_test, y_test):\n",
    "    print(\"---K-nearest Neighbor Algorithm (KNN)---\")\n",
    "    knnc = KNeighborsClassifier()\n",
    "    knnc.fit(X_train, y_train)\n",
    "    y_pred = knnc.predict(X_test)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"\")\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def detection_gnb(X_train, y_train, X_test, y_test):\n",
    "    print(\"---Gaussian Naive Bayes---\")\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def detection_kmeans(X_train, y_train, X_test, y_test, num_clusters):\n",
    "    print(\"---K-means Clustering---\")\n",
    "    kmc = KMeans(n_clusters = num_clusters)\n",
    "    kmc.fit(X_train)\n",
    "    y_pred = kmc.predict(X_test)\n",
    "\n",
    "    # print(y_test)\n",
    "    # print(y_pred)\n",
    "    y_processed = []\n",
    "    ss = set()\n",
    "    for yi in y_pred:\n",
    "        ss.add(yi)\n",
    "        if yi == 0:\n",
    "            y_processed.append(0)\n",
    "        else:\n",
    "            y_processed.append(1)\n",
    "    print(\"ss: \", ss)\n",
    "    precision = metrics.precision_score(y_test, y_processed)\n",
    "    recall = metrics.recall_score(y_test, y_processed)\n",
    "    f1 = metrics.f1_score(y_test, y_processed)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_processed)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def detection_rf(X_train, y_train, X_test, y_test):\n",
    "    print(\"---Random Forest---\")\n",
    "    rfc = RandomForestClassifier()\n",
    "    rfc.fit(X_train, y_train)\n",
    "    y_pred = rfc.predict(X_test)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x:  5159\n",
      "#features:  5000\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.4\n",
    "NUM_FEATURE = 5000\n",
    "\n",
    "x = feature_extraction_bagofwords(emails_list, NUM_FEATURE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Support Vector Machines Algorithm---\n",
      "precision:  0.9774696707105719\n",
      "recall:  0.9724137931034482\n",
      "f1:  0.9749351771823681\n",
      "accuracy:  0.9859496124031008\n",
      "\n",
      "\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "\n",
      "precision:  0.9730700179533214\n",
      "recall:  0.9344827586206896\n",
      "f1:  0.953386103781882\n",
      "accuracy:  0.9743217054263565\n",
      "\n",
      "\n",
      "---Gaussian Naive Bayes---\n",
      "precision:  0.9376083188908145\n",
      "recall:  0.9327586206896552\n",
      "f1:  0.9351771823681937\n",
      "accuracy:  0.9636627906976745\n",
      "\n",
      "\n",
      "---K-means Clustering---\n",
      "ss:  {0, 1, 2}\n",
      "precision:  0.0\n",
      "recall:  0.0\n",
      "f1:  0.0\n",
      "accuracy:  0.4748062015503876\n",
      "\n",
      "\n",
      "---Random Forest---\n",
      "precision:  0.9417637271214643\n",
      "recall:  0.9758620689655172\n",
      "f1:  0.9585097375105843\n",
      "accuracy:  0.9762596899224806\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detection_svm(X_train, y_train, X_test, y_test)\n",
    "detection_knn(X_train, y_train, X_test, y_test)\n",
    "detection_gnb(X_train, y_train, X_test, y_test)\n",
    "detection_kmeans(X_train, y_train, X_test, y_test, 3)\n",
    "detection_rf(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of x:  5159\n"
     ]
    }
   ],
   "source": [
    "TEST_SIZE = 0.4\n",
    "\n",
    "x = feature_extraction_vo(emails_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Support Vector Machines Algorithm---\n",
      "precision:  0.8040201005025126\n",
      "recall:  0.27586206896551724\n",
      "f1:  0.41078305519897307\n",
      "accuracy:  0.7776162790697675\n",
      "\n",
      "\n",
      "---K-nearest Neighbor Algorithm (KNN)---\n",
      "\n",
      "precision:  0.5408560311284046\n",
      "recall:  0.2396551724137931\n",
      "f1:  0.3321385902031064\n",
      "accuracy:  0.7291666666666666\n",
      "\n",
      "\n",
      "---Gaussian Naive Bayes---\n",
      "precision:  0.4439252336448598\n",
      "recall:  0.16379310344827586\n",
      "f1:  0.23929471032745595\n",
      "accuracy:  0.7073643410852714\n",
      "\n",
      "\n",
      "---K-means Clustering---\n",
      "ss:  {0, 1, 2}\n",
      "precision:  0.251357220412595\n",
      "recall:  0.7982758620689655\n",
      "f1:  0.3823286540049546\n",
      "accuracy:  0.2751937984496124\n",
      "\n",
      "\n",
      "---Random Forest---\n",
      "precision:  0.8793103448275862\n",
      "recall:  0.4396551724137931\n",
      "f1:  0.5862068965517242\n",
      "accuracy:  0.8255813953488372\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detection_svm(X_train, y_train, X_test, y_test)\n",
    "detection_knn(X_train, y_train, X_test, y_test)\n",
    "detection_gnb(X_train, y_train, X_test, y_test)\n",
    "detection_kmeans(X_train, y_train, X_test, y_test, 3)\n",
    "detection_rf(X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv3",
   "language": "python",
   "name": "senv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
