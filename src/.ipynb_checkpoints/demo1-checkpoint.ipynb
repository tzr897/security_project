{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token_1.py is OK for milestone1\n",
    "\n",
    "This file is still being improved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b393cb8b4d5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocabularyProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow\n",
    "from tflearn.data_utils import VocabularyProcessor\n",
    "import json\n",
    "\n",
    "enron_data_path = \"../dataset/enron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_one_set(data_path):\n",
    "    res = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file_name in files:\n",
    "            cur_file_path = os.path.join(root, file_name)\n",
    "            cur_txt = \"\"\n",
    "            with open(cur_file_path) as cur_f:\n",
    "                try:\n",
    "                    cur_lines = cur_f.readlines()\n",
    "                    for one_line in cur_lines:\n",
    "                        one_line = one_line.strip('\\n')\n",
    "                        one_line = one_line.strip('\\r')\n",
    "                        cur_txt += one_line\n",
    "                    res.append(cur_txt)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "    return res\n",
    "\n",
    "def process_all_data():\n",
    "    spam_list = []\n",
    "    ham_list = []\n",
    "    NUM_SET = 1\n",
    "    for i in range(1, NUM_SET + 1):\n",
    "        spam_path = enron_data_path + str(i) +  \"/spam/\"\n",
    "        ham_path = enron_data_path + str(i) + \"/ham/\"\n",
    "        cur_spam_list = preprocess_one_set(spam_path)\n",
    "        cur_ham_list = preprocess_one_set(ham_path)\n",
    "        spam_list += cur_spam_list\n",
    "        ham_list += cur_ham_list\n",
    "    return spam_list, ham_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of spam_list:  1487\n",
      "len of ham_list:  3672\n"
     ]
    }
   ],
   "source": [
    "spam_list, ham_list = process_all_data()\n",
    "emails_list = spam_list + ham_list\n",
    "len_spam = len(spam_list)\n",
    "len_ham = len(ham_list)\n",
    "print(\"len of spam_list: \", len_spam)\n",
    "print(\"len of ham_list: \", len_ham)\n",
    "y = [1] * len_spam + [0] * len_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_bagofwords(emails_list, num_features):\n",
    "    # currently do not consider max_features\n",
    "    tfidfv = TfidfVectorizer(\n",
    "        decode_error = \"ignore\",\n",
    "        analyzer = \"word\",\n",
    "        stop_words = \"english\",\n",
    "        smooth_idf = False,\n",
    "        max_features = num_features,# at most 2010 features for two datasets\n",
    "    )\n",
    "    x = np.array(emails_list)\n",
    "    x = tfidfv.fit_transform(x)\n",
    "    x = x.toarray()\n",
    "    vocab_path = \"../output/vocabulary_bagofwords.txt\"\n",
    "    print(\"output the vocabulary to \" + vocab_path + \" ......\\n\")\n",
    "    # with open(vocab_path, 'w') as f:\n",
    "    #     f.write(json.dumps(tfidfv.vocabulary_))\n",
    "    print(\"len of x: \", len(x))\n",
    "    print(\"#features: \",len(x[0]))\n",
    "    return x\n",
    "\n",
    "\n",
    "def feature_extraction_vo(emails_list):\n",
    "    vp = VocabularyProcessor(\n",
    "        max_document_length = 1000,\n",
    "        min_frequency = 1,\n",
    "        vocabulary = None,\n",
    "        tokenizer_fn = None\n",
    "    )\n",
    "    x = vp.fit_transform(emails_list)\n",
    "    x = np.array(list(x))\n",
    "    print(x)\n",
    "    vocab_path = \"../output/vocabulary_tf.txt\"\n",
    "    # with open(vocab_path, 'w') as f:\n",
    "    #     f.write(json.dumps(vp.vocabulary_._mapping))\n",
    "    print(\"len of x: \", len(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output the vocabulary to ../output/vocabulary_bagofwords.txt ......\n",
      "\n",
      "len of x:  5159\n",
      "#features:  2000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "TEST_SIZE = 0.4\n",
    "NUM_FEATURE = 2000\n",
    "\n",
    "x = feature_extraction_bagofwords(emails_list, NUM_FEATURE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.9770318021201413\n",
      "recall:  0.9534482758620689\n",
      "f1:  0.9650959860383943\n",
      "accuracy:  0.9806201550387597\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def detection_svm():\n",
    "    svm_c = svm.SVC()\n",
    "    svm_c.fit(X_train, y_train)\n",
    "    y_pred = svm_c.predict(X_test)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "\n",
    "detection_svm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.9392361111111112\n",
      "recall:  0.9327586206896552\n",
      "f1:  0.9359861591695502\n",
      "accuracy:  0.9641472868217055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def detection_gnb():\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "\n",
    "detection_gnb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def detection_knn():\n",
    "    knnc = KNeighborsClassifier()\n",
    "    knnc.fit(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv3",
   "language": "python",
   "name": "senv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
